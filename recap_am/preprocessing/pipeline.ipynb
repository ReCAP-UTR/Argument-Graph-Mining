{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "#!python -m spacy download de_core_news_md\n",
    "#!python -m spacy download en_core_web_lg \n",
    "en_nlp = spacy.load(\"en_core_web_lg\")\n",
    "#de_nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "##from nltk import pos_tag, pos_tag_sents, word_tokenize, sent_tokenize\n",
    "##from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump, load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_deutsch = pd.read_csv(\"deutsch_stances.csv\", index_col = 0)\n",
    "df_deutsch.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_english = pd.read_csv(\"english_stances.csv\", index_col = 0)\n",
    "df_english.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stems = []\n",
    "        for item in tokens:\n",
    "            if item.isdigit():\n",
    "                continue\n",
    "            elif item.isalnum():\n",
    "                stems.append(PorterStemmer().stem(item))\n",
    "        return stems\n",
    "\n",
    "def clean_text(text):\n",
    "    website_pattern = re.compile(r'\\((.*?)\\)') \n",
    "    slash_pattern = re.compile(r'[\\[\\]]')\n",
    "    text = re.sub(website_pattern, \"\", text)\n",
    "    text = re.sub(slash_pattern, \"\", text)\n",
    "    return text\n",
    "\n",
    "def generate_base(df, column, language, model = \"glove\"):\n",
    "    lang = \"en\" if language == \"english\" else \"de\"\n",
    "    \n",
    "    if model == \"glove\":\n",
    "        nlp = en_nlp if language == \"english\" else de_nlp\n",
    "        embeddings = np.array([nlp(x).vector for x in list(df[column].values)])\n",
    "        shape = embeddings.shape[1]\n",
    "        columns = [\"{}_dimension_{}\".format(column, i) for i in range(shape)]\n",
    "        ff = pd.DataFrame(data=embeddings, columns=columns)\n",
    "        \n",
    "    elif model == \"tfidf-stemmed\":\n",
    "        model = load(\"tfidf_\"+ lang +\"_stemmed.sav\")\n",
    "        data = model.transform(df[column])\n",
    "        columns = [column +\":\" + col for col in model.get_feature_names()]\n",
    "        ff = pd.SparseDataFrame(data = data, columns=columns).fillna(0)\n",
    "        \n",
    "    elif model == \"tfidf-unstemmed\":\n",
    "        model = load(\"tfidf_\"+ lang +\"_unstemmed.sav\")\n",
    "        data = model.transform(df[column])\n",
    "        columns = [column +\":\" + col for col in model.get_feature_names()]\n",
    "        ff = pd.SparseDataFrame(data = data, columns=columns).fillna(0)\n",
    "        \n",
    "    return ff\n",
    "\n",
    "def generate_additional(df, column, language, modes = [\"pos\", \"ner\", \"sentiment\"]):\n",
    "    nlp = en_nlp if language == \"english\" else de_nlp\n",
    "    docs = [nlp(x) for x in list(df[column].values)]\n",
    "    n = len(df)\n",
    "    dfs = []\n",
    "    \n",
    "    # use pos tags\n",
    "    if \"pos\" in modes:\n",
    "        # research tag by using spacy.explain: spacy.explain(\"ADP\")\n",
    "        pos_tags = {\"PRON\": [0]*n, \n",
    "                    \"ADV\": [0]*n, \n",
    "                    \"ADJ\": [0]*n, \n",
    "                    \"ADP\": [0]*n,\n",
    "                    \"DET\": [0]*n,\n",
    "                    \"AUX\": [0]*n,\n",
    "                    \"VERB\": [0]*n, \n",
    "                    \"NOUN\": [0]*n, \n",
    "                    \"PUNCT\": [0]*n, \n",
    "                    \"NUM\": [0]*n}\n",
    "\n",
    "        for i, doc in enumerate(docs):\n",
    "            for token in doc:\n",
    "                if token.pos_ in pos_tags.keys():\n",
    "                    pos_tags[token.pos_][i] += 1\n",
    "        tf = pd.DataFrame.from_dict(pos_tags)\n",
    "        tf.columns = [column +\":\" + col for col in tf.columns]\n",
    "        dfs.append(tf)\n",
    "    \n",
    "    # use sentiment tas: negative, neutral, positive and compound\n",
    "    if \"sentiment\" in modes:\n",
    "        sentiment = [sid.polarity_scores(x) for x in list(df[column].values)]\n",
    "        tf = pd.DataFrame(data=sentiment)\n",
    "        tf.columns = [column +\":\" + col for col in tf.columns]\n",
    "        dfs.append(tf)\n",
    "        \n",
    "    # use named entity recognition:\n",
    "    if \"ner\" in modes:\n",
    "        ner_types = {\"PERSON\": [0]*n, \n",
    "                    \"NORP\": [0]*n, \n",
    "                    \"FAC\": [0]*n, \n",
    "                    \"ORG\": [0]*n,\n",
    "                    \"GPE\": [0]*n,\n",
    "                    \"LOC\": [0]*n,\n",
    "                    \"PRODUCT\": [0]*n, \n",
    "                    \"EVENT\": [0]*n, \n",
    "                    \"WORK_OF_ART\": [0]*n, \n",
    "                    \"LAW\": [0]*n,\n",
    "                    \"LANGUAGE\": [0]*n, \n",
    "                    \"QUANITY\": [0]*n,\n",
    "                    \"ORDINAL\": [0]*n, \n",
    "                    \"CARDINAL\": [0]*n}\n",
    "        for i, doc in enumerate(docs):\n",
    "            for entity in doc.ents:\n",
    "                if entity.label_ in ner_types.keys():\n",
    "                    ner_types[entity.label_][i] += 1\n",
    "        tf = pd.DataFrame.from_dict(ner_types)\n",
    "        tf.columns = [column +\":\" + col for col in tf.columns]\n",
    "        dfs.append(tf)\n",
    "    if \"structure\" in modes:\n",
    "        pass\n",
    "    \n",
    "    return pd.concat(dfs, axis=1)\n",
    "\n",
    "def prep_dataset(df, model, language, modes = []):\n",
    "    dfs = []\n",
    "    df_stance = pd.concat([df['stance']], axis=1)\n",
    "    df_stance['stance'] = df_stance.stance.apply(lambda x: 1 if x == \"RA\" else 0)\n",
    "    dfs.append(df_stance)\n",
    "    #dfs.append(generate_base(df, \"child_text\", model=model, language=language))\n",
    "    #dfs.append(generate_base(df, \"parent_text\", model=model, language=language))\n",
    "    if modes != []:\n",
    "        dfs.append(generate_additional(df, \"child_text\", language=language, modes = modes))\n",
    "        dfs.append(generate_additional(df, \"parent_text\", language=language, modes = modes))\n",
    "    return pd.concat(dfs, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = prep_dataset(df_english[:10000], model = \"glove\", language=\"english\", modes = [])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "# random state value\n",
    "rsv = 42\n",
    "# cpus used for training\n",
    "n_jobs = -1 \n",
    "\n",
    "models = {\"SVG\": SVC(probability=True,\n",
    "                    random_state=rsv),\n",
    "          \"LogReg\": LogisticRegression(random_state=rsv,\n",
    "                                      n_jobs=n_jobs),\n",
    "          \"RanFor\": RandomForestClassifier(random_state=rsv,\n",
    "                                          n_jobs=n_jobs),\n",
    "          \"GausNB\": GaussianNB(),\n",
    "          \"LDA\": LinearDiscriminantAnalysis(),\n",
    "          \"KNN\": KNeighborsClassifier(n_jobs=n_jobs)}\n",
    "\n",
    "# split in training data matrix X and target y\n",
    "def generate_cv_sets(df: pd.DataFrame):\n",
    "    X = df.loc[:, df.columns != 'stance']\n",
    "    y = df[['stance']].values.ravel()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_cv_sets(df)\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train.fillna(0), y_train)\n",
    "    score = model.score(X_test.fillna(0), y_test)\n",
    "    cf_matrix = metrics.confusion_matrix(y_test, model.predict(X_test.fillna(0)))\n",
    "    results[name] = {\"score\": score, \"cfm\": cf_matrix}\n",
    "    models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = generate_cv_sets(df)\n",
    "results = {}\n",
    "model = LogisticRegression(random_state=42, n_jobs=-1)\n",
    "model.fit(X_train.fillna(0), y_train)\n",
    "score = model.score(X_test.fillna(0), y_test)\n",
    "cf_matrix = metrics.confusion_matrix(y_test, model.predict(X_test.fillna(0)))\n",
    "score, cf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
